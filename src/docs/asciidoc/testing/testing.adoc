[#testing]
== Testing

[quote, Bertrand Meyer]
Testing a program to assess its quality is, in theory, akin to sticking pins into a dollâ€”very small pins, very large doll.

Writing software is a creative process. Thoughts materialize to artifacts which provide some value to somebody.

If one overcomes the first hurdles of learning the basics of software development, programming, communication and tooling, one will experience an enormous joy in writing software. More sooner than later one will also discover that software can be a beast which is sometimes very hard to tame.

In fact, software tests help in managing the complexity of those giant buildings made up of mere thoughts, rules, and conventions.

Typically, without tests there is a tendency to not change the behavior of the system in a fundamental way. That means that - given the system is interesting enough to be maintained over a longer period of time - the software system will be extended with various features.

Adding features without broadening the scope of the system, without rethinking structures in a radical way or without extracting common behaviour to so called core components will end in exponential growth of costs for writing new features and for maintaining them.

Sooner than later features will interact in a way which contradict themselves, and as a software developer one is forced to rewrite parts of the system which play a major role in the software. that means changing the class or function which is used everywhere.

It is a vicious circle: if some software doesn't have tests, it is dangerous to change anything in the codebase, since one cannot be sure about if the change didn't have side effects which were not intended.

If you negate the positive effects of tests during development, this will be the point when tests would be handy.

Writing them at a later stage of a project is a very hard problem since if the software was not written with tests in mind it is downright impossible to introduce them.

Parts of the system have to be made 'testable' which requires experience, diligence and endurance.

It is again an art by itself. It is time consuming and thus expensive. By far more expensive compared to the approach of writing them incrementally, like the system itself.

=== Manual Testing

Programs need to adhere to specifications to serve their purpose. That means that the result of an execution of a program is well defined and observable.

The simple form of testing, nevertheless a very important one,  is the so called manual testing approach.

Executing this form of test doesn't necessarily involve any knowledge of computer programming. Typically, one compares the outcome of a program with its specification. Depending on the completeness of this specification, one can identify differences and thus report them.

The downside of this approach is that it is very time consuming to perform a full testing cycle, and it involves human interaction which makes it expensive. Nevertheless it is important that programs are tested by humans before they ship to the customers, since no automated test can replace the experience of a good quality assurance testing team.

Sometimes, the complexity of a system makes it very hard to programmatically test the whole system, in some cases it is downright impossible. In such cases manual testing is invaluable.

=== White vs Blackbox tests

It makes a difference if you know details about the implementation, or if you test a system just by observing its behavior and derive certain properties from it. The first approach is called 'Whitebox', the latter 'Blackbox' testing.

=== Tests serve as documentation

Tests have many advantageous properties, and one of them is that they can also be seen as a documentation. They describe how a system works, which inputs it uses, which outputs are expected.

Seen from this perspective, tests give a invaluable insight in the function of a system.



=== Unit Tests

Software is developed in small chunks which are combined to form a more complex system. Those chunks, wherever one may draw the line, have to be correct in order to combine them to bigger and more complex systems.

Such 'units' of software should be tested in isolation - such tests are called 'Unit tests'.

Unit tests provide ideally the minimum environment for a software chunk to properly work and demonstrate its correct behavior.

This isolates the code chunk from the rest of the system, it can be fed with specialized input and the output of the chunk is tested against a set of expectations.

This pinpoints the functionality, using this approach one gets executable specifications. Such specifications have the great advantge that they are strongly coupled to the code. Changing one's expectations about the functionality of a small software part directly and unambiguously affects the test outcome. One has to alter the software chunk in order to adhere to the new rules. From that point onwards, the specification - if executed on a regular basis - gives feedback about the proper function of this part of the system.

Unit tests are lightweight and fast in execution. This is a very important trait of good unit tests. It is important because ideally unit tests are executed with every change to the source code.

Instant feedback is key - if the tests are failing a developer is forced to change their contribution in a way that the existing code base is following the <<open-closed-principle, "Open - Closed - Principle">>.

A synonym in the JVM world for unit tests is 'JUnit'. In fact, this library has extended its success to other languages, and as such there is support for this 'xUnit' approach for several languages. This is also a clear sign that the approach to unit testing is of great value.footnote:[There are many examples for cross - insemination of libraries, a very successful example would be the 'rx library' family, which has been a major driver for innovation on various platforms. It was originally conceived for the .NET platform, and has been ported to the JVM or Javascript runtime as well. Skim through http://reactivex.io/languages.html[reactiveX.io] for more information about that.]

Anyway, the 'JUnit' library supports all traits mentioned above, like providing means for setting up an isolated environment for testing a software chunk, deallocating resources afterwards, executing tests in parallel, embedding test execution in a continuous integration environment and so on. footnoteref:[It is pretty standard to use JUnit on the JVM, however we will use ScalaTest, another unit testing library, which is very common when programming in Scala. It supports JUnit style tests as well as a number of other testing approaches.]

A critique on unit tests is that they slow down development significantly. This is true, and one has to keep attention to the quality of unit tests as well. Tests tend to be written in a 'fire and forget' style. One has to constantly remind oneself that quality of a test matters - at least it should be written with the same amount of care and dedication like the production code, if not more.

The investment in your tests, their documentation, structure and design pays off as soon as the first test is failing: If it was written cautiously and with foresight, it is easy to spot the problem. It is easy to see what is going on. What is needed to change is clear and easy to fix.

If your tests however are written in a poor way, you will spend a lot of time debugging your code, not remembering what was the implementations original intention. It will be a major undertaking to get tests passing again. Often, during this process, one tends to rewrite the test again, just to understand what was going on.

NOTE: Choose wisely which tests you write, don't underestimate the maintenance burden of poorly written unit or component tests.

See also <<test-coverage, the chapter about test coverage>> for a discussion about the 'right amount of unit tests'.


=== Component Tests

Component tests glue the aforementioned software chunks together to more complex, bigger software systems.

Those components have to be tested as well, that means the interaction between different layers of software has to be verified.

Here, similar techniques can be applied. In fact one can apply the same approach for writing component tests than for writing unit tests. There is not much difference. Maybe they are harder to implement since component tests need more setup. Component tests typically need an environment to run in, which may not be easy to provide or simulate.

If component testing (or unit testing for that matter) is strictly applied, the observable behavior of software components increases. Interfaces get defined more clearly. Typically, components themselves are broken down into smaller parts during the process of testing. Components become  smaller in size and thus reach a better quality. Interfaces improve and reach a better stability.

Writing component tests often involves simulating certain parts of the system. This technique is called 'Mocking'. Again, there exist libraries for simulating behavior. 'Mocking' means to simulate API calls or to measure if some call was performed when using an interface. A disadvantage of mocking is however that it is mirroring the implementation and thus very brittle and cumbersome to maintain.

A rule of thumb is that a well thought software system is easy to test, and also easy to simulate. That doesn't mean that it is not hard work to do so. Writing good tests, no matter if they are called unit or component tests, is a great challenge and often more creative work than writing the implementation itself.


=== Performance Tests

Performance tests concentrate on the response time of the system, how resilient a system is or how it behaves under load. Repeatable performance tests are very hard to write and to maintain. The reason for that is that the system which is under test (SUT) continuously changes and evolves. This means that the performance metrics are hard to compare, so one has to choose wisely in order to be able to measure progress or regressions.

Still, performance tests also demand that the software is executed under comparable conditions, which means that performance tests should be executed in isolation. No other process should compete with the resources used by the SUT. A dedicated performance testing environment is the only answer to this requirment.

[#test-coverage]
=== Test coverage

Line test coverage reports tell you which lines of code were executed by your tests. Line coverage reports are very easy to understand (a line was hit by a test or not) and of great help to write tests themselves. In modern IDE's such line test coverage reports are already included (as well as the ability to execute tests inline). Like this it is easy to see which parts of a class for example are still untested and which one is already hit once by a test.

'Hitting' in this context means that a conceptualized program counter passes by this line and uses the line for executing. Such line coverage reports can also be used to detect dead code, relative to a usecase and can be useful in this respect as well.

Following figure gives you an example of a line test coverage report.

.Example test line coverage report
image::test-line-coverage.png[Example test line coverage report,744,514]
<<<

Another form of a test coverage report is to summarize code by certain criterias. An example for such an criteria is to group code by files, by packages or modules. Of course it depends in which environment (programming language, library, framework ... ) a system is written. A test coverage report will give you insights which parts of the system need more attention.

.Example test coverage report
image::test-coverage-example.png[Example test coverage report, 1357,453]

NOTE: even with 100% coverage, bugs still can exist in the code.

100% coverage is also very hard to reach, and in most cases just not necessary. The best ratio for test coverage should be decided individually. Typically though a test coverage report will say that there is not enough test coverage and it is time for writing tests.

NOTE: Creating the infrastructure to perform performance tests or coverage reports is not a negligible effort. Such reports however are a waste of time and resources if they are not constantly monitored and actions and reactions are derived from interpreting those reports.

=== Costs of tests

If you think about it, just by reading this chapter, you will come to the conclusion that software tests are expensive. They are expensive to write and to maintain.

There is no discussion about that certain kinds of tests help writing software more effectively. Software systems tend to reach a complexity which is impossible to handle without the safety net of tests.

Studies have shown that the additional effort for writing unit tests is about 30% of additional time to write the tests. This investment only pays off if the bug rate has decreased significantly by using the test driven approach. Another reason for creating unit tests is to be able to refactor the code and be sure that the system still adheres to the specification.

=== Usability Testing

Apps are more successful if they have a good usability, this aspect of software engineering must not be underestimated. An application has to be beautiful, fast and correct. The user sees those traits in this sequence, in fact the programmer should strive to implement them the other way around.

NOTE: it is much easier to make a correct system faster, than a fast system correct. A fast but incorrect system is useless.

A good architecture of an application allows to separate the GUI aspect from the 'business logic' of an application. The GUI should be usable, beautiful, efficient. To reach this goal constant feedback from end users is very important, early prototypes can help here, early feedback loops help to go in the right direction.


=== Best practices for testing

- write tests early, during development (TDD)
- avoid complexity by splitting the system up in smaller parts
- automate test execution
- if a bug is identified, write a test for it
- refactor tests
- test code should have the same quality like production code
- measure your tests (how many are there, executed daily, how many are ignored, ... )
- document tests for future readers of those tests (will be very valuable to have documentation on the 'intention' of the test)

=== Unit Testing in Scala - with Scalatest

Unit testing is not bound to a specific framework or library. There are quite a few libraries out there which make writing Unit Tests easy.

During this course we will stick to the http://www.scalatest.org[ScalaTest] library.

Here is an example of a unit test written in ScalaTest:

[source, scala]
----
import collection.mutable.Stack
import org.scalatest._

class ExampleSpec extends FlatSpec with Matchers {

  "A Stack" should "pop values in last-in-first-out order" in {
    val stack = new Stack[Int]
    stack.push(1)
    stack.push(2)
    stack.pop() should be (2)
    stack.pop() should be (1)
  }

  it should "throw NoSuchElementException if an empty stack is popped" in {
    val emptyStack = new Stack[Int]
    a [NoSuchElementException] should be thrownBy {
      emptyStack.pop()
    }
  }
}
----

This library supports a variety of testing approaches, for simplicity we will also stick just to one subset of writing tests ("WordSpec").

In this code example you can also witness a <DSL>> at work, the flexible and _scalable_ architecture of the Scala language makes it a good host for domain specific languages.

Lets have a look at the first section of the **Spec**ification:

[source, scala]
----

...

  "A Stack" should "pop values in last-in-first-out order" in {
    val stack = new Stack[Int]
    stack.push(1)
    stack.push(2)
    stack.pop() should be (2)
    stack.pop() should be (1)
  }

...

----

By enriching the type _String_ the ScalaTest library defines the methods
_should_ and _in_ on this data type. This advanced technique of enriching types with custom methods is the basis of creating powerful DSLs.

What this means is that a library author, without changing the language compiler itself, can add custom behavior to Scala's syntax, and the ScalaTest library authors just did this in a very clever way.

As users of the library we don't have to think too much about this, we can just profit from this fact and work with tests which read themselves much like prose.

If you look at the example, without knowing Scala, you get an idea what is going on. footnote:[This is what a DSL is for - hide complexity, concentrate on the problem at hand.] A datatype called _Stack_ (which contains values of type _Int_) is described, with its methods _push_ and _pop_. The latter are closely related to each other. By looking at the _specification_ you can derive the behavior of the implementation, thus the _Specification_ also serves as a _documentation_ of the functionality.

